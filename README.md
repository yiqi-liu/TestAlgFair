# TestAlgFair
The folder `replication-package` inclues the data and code needed for replicating the Monte Carlo simulation and empirical application in Section 8 and Appendix C of [Liu and Molinari (2025)](https://arxiv.org/abs/2402.08879). With proper multithreading configurations, the replication takes about 8 days to finish on two single-node SLURM clusters with 256 and 112 cores, respectively, and
```
> sessionInfo()
R version 4.4.3 (2025-02-28)
Platform: x86_64-pc-linux-gnu
Running under: Rocky Linux 9.5 (Blue Onyx)

Matrix products: default
BLAS/LAPACK: /usr/lib64/libopenblas-r0.3.26.so;  LAPACK version 3.12.0
```
### TL;DR
Before running each `.R` file, make sure the working directory is set to the directory of the current `.R` file.
- To replicate Figure 5, run `simulation/code/simulation-DGPplot.R`. Results will be saved as `simulation/results/simulation-DGPplot.html`. 
- To replicate the simulation results in Table I, for `[size]`$\in${1, 5, 10} denoting the sample size in thousands:
  * run `simulation/code/test1-weak-skew/test1n[size]k.R` for the top panel reporting the weak group skew test results;
  * run `simulation/code/test2-LDA/test2n[size]k.R` for the middle panel reporting the LDA test results;
  * run `simulation/code/test3-FDistance/test3n[size]k.R` for the bottom panel reporting the Distance-to-F test results.

  If running on a SLURM cluster, the output directory is specified in the `run.slurm` file contained in folders `test1-weak-skew`, `test2-LDA`, and `test3-FDistance`, and outputs are saved as `.txt` files to the corresponding `simulation/results` directory. If running on PC, results will be printed directly in the RStudio console.

- To replicate Figures 6-7 and Table II, run `application/code/rf/hypTest-rf.R`. Results will be saved as `application/results/hypTest-results/rf/hypTest-rf.html`.
  * To replicate Figures C.1-C.2 and Table C.1, run `application/code/lasso/hypTest-lasso.R`. Results will be saved as `application/results/hypTest-results/lasso/hypTest-lasso.html`
 
- To replicate Figure 8 and Table III, run `application/code/rf/buildAlg-rf.R`. Results will be saved as `application/results/buildAlg-results/rf/alt-alg-f1a.png` (Figure 8) and `application/results/buildAlg-results/rf/table_frac_bl_trt.csv` (Table III).
  * To replicate Figure C.3 and Table C.II, run `application/code/lasso/buildAlg-lasso.R`. Results will be saved as `application/results/buildAlg-results/lasso/alt-alg-f1a.png` (Figure C.3) and `application/results/buildAlg-results/lasso/table_frac_bl_trt.csv` (Table C.II).

- To replicate Tables C.III and C.IV, run `application/code/variability.R`. Results will be saved as `application/results/rep-results/variability.html`.

## Data Structure
- File `all-func.R` includes all functions needed to implement the statistical procedures proposed in the paper. By default, `all-func.R` expects multiple CPUs for parallelization via
  ```
  spec_cpu <- as.integer(Sys.getenv("OMP_NUM_THREADS"))
  torch_set_num_threads(spec_cpu)
  torch_set_num_interop_threads(ceiling(log(spec_cpu)))
  library(doMC)
  registerDoMC(cores=spec_cpu)
  ```
  Comment out this block if `OMP_NUM_THREADS` is not explicitly set (if running on a SLURM cluster, this parameter is set by specifying `export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK` in the SLURM file).
- Folder `application` includes relevant files for running the empirical application:
    * folder `code` contains code that generates the results in tables and figures. It has two sub-folders, `lasso` and `rf`, each including the following files for the corresponding `[method]`$\in${`"lasso"`, `"rf"`} used for estimating the nuisance parameters:
        - `hypTest-[method].R` for implementing the three hypothesis tests introduced in Section 6. It calls `../results/hypTest-results/hypTest-[method].Rmd` to generate `../results/hypTest-results/hypTest-[method].html` that contains the results in Table II and Figures 6-7 for `[method]`=`"rf"`, and Figures C.1-C.2 and Table C.1 for `[method]`=`"lasso"`;
        - `buildAlg-[method].R` for building new algorithms on the fairness-accuracy frontier introduced in Section 5.4 that generates Figure 8 in `../../results/buildAlg-results/rf/alt-alg-f1a.png` and Table III in `../../results/buildAlg-results/rf/table_frac_bl_trt.csv` for `[method]`=`"rf"`, and Figure C.3 in `../../results/buildAlg-results/lasso/alt-alg-f1a.png` and Table C.II in `../../results/buildAlg-results/lasso/table_frac_bl_trt.csv` for `[method]`=`"lasso"`;
        - `rep-[method].R` for repeating the empirical exercise 20 times to assess the variability of the empirical results due to randomness in the nuisance estimators. Results are saved to `../../results/rep-results/rep-[method].csv`
          
      Folder `code` also includes the file `variability.R` that uses the `.csv` results generated by `[method]/rep-[method].R` to produce Tables C.III and C.IV in `../results/rep-results/variability.html`;
    * folder `results` collects the output from running the files in `code`;
        - folder `hypTest-results` collects the produced `hypTest-[method].html` files from calling `../code/[method]/hypTest-[method].R`;
        - folder `buildAlg-results/[method]` collects `.png` figures and `.csv` files generated from running `../code/[method]/buildAlg-[method].R`;
        - folder `rep-results` collects the produced `variability.html` file from calling `../code/variability.R`;
    * folder `obermeyer-code` contains code adapted from [Obermeyer et al. (2019)](https://gitlab.com/labsysmed/dissecting-bias):
        - folder `dissecting-bias-modified` is a copy of [Obermeyer et al. (2019)](https://gitlab.com/labsysmed/dissecting-bias) that contains 2 additional files in the sub-directory `code/model`, `main_modified.py` and `model_modified.py`, modified based on the orginal files `main.py` and `model.py` in the same sub-directory. Any modification is documented in the corresponding `.py` file;
        - folder `plot0` is the source code of `dissecting-bias-modified/plot0_0.1.tar.gz`;
        - file `Obermeyer-Fig1b.R` is adapted from `dissecting-bias-modified/code/figure1/figure1b.R`;
    * folder `data` contains `.csv` data files generated from running `obermeyer-code/dissecting-bias-modified/code/model/main_modified.py`:
        - file `all_Y_x_df.csv` includes the final set of Y and X used for building predictive algorithms;
        - files named `pred_[outcome]_df.csv` are predictions at all observations from the three experimental algorithms discussed in Table 2 of Obermeyer et al. (2019), where `[outcome]`$\in$`{"gagne_sum_t", "log_cost_avoidable", "log_cost"}` corresponding to, respectively, the number of active chronic conditions, avoidable costs, and total costs.

- Folder `simulation` includes the following sub-folders for running the simulations in Section 8.1: 
    * folder `code` contains code that generates the results reported in Table 1: for `[size]`$\in${1, 5, 10} denoting the sample size in thousands,
      - folder `test1-weak-skew` includes files `test1n[size]k.R` for producing results of the weak group skew test;
      - folder `test2-LDA` includes files `test1n[size]k.R` for producing results of the LDA test. Because the LDA test takes much longer to run, the 1000 Monte Carlo replications are split into four files for `[size]`=5, each running 250 replications, and eight files for `[size]`=10, each running 125 replications, with the corresponding `.R` files contained in two sub-folders:
        * folder `test2n5k-split` contains files `test2n5k-[seed].R` for `[seed]`$\in${0, 1, 2, 3} denoting the random seed used for each of the four 250 replications that generate `test2n5k-[seed].csv` files in the same directory;
        * folder `test2n10k-split` contains files `test2n10k-[seed].R` for `[seed]`$\in${0, 1, 2, 3, 4, 5, 6, 7} denoting the random seed used for each of the eight 125 replications that generate `test2n10k-[seed].csv` files in the same directory;
      - folder `test3-FDistance` includes files `test3n[size]k.R` for producing results of the distance-to-F test;
      - file `simulation-DGPplot.R` calls `../results/simulation-DGPplot.Rmd` to generate `../results/simulation-DGPplot.html` that replicates Figure 5, and files `../truth_balance.csv` and `../truth_rskew.csv` for use in the other simulation scripts;
     * folder `results` collects all output results if the output directory is specified in the corresponding `run.slurm` contained in folders `../code/test1-weak-skew`, `../code/test2-LDA`, and `../code/test3-FDistance` when run on a SLURM clsuter. If running on PC, results will be printed directly in the RStudio console.
    


## Note on Reproducibility

We note that results based on random forests trained using the [`grf`](https://grf-labs.github.io/grf/index.html) package are not guaranteed to be reproducible across platforms, even if the same `R` seed is used; see the discussion [here](https://grf-labs.github.io/grf/REFERENCE.html#forests-predict-different-values-depending-on-the-platform-even-though-the-seed-is-the-same). 
Tests involving optimization via stochastic gradient descent implemented by the [`torch`](https://torch.mlverse.org/) package are also not reproducible across platforms even after setting the same seed (see the discussion [here](https://github.com/mlverse/torch/issues/1311)), which will affect the reproducibility of the $F$ estimate---and consequently the distance-to-F test in both the simulation study and the empirical application. 

For these reasons, we assess and report the variability of the empirical results by repeating the entire empirical exercise 20 times (see Appendix C of the paper). For the simulation study, we expect cross-platform variability to be trivial, as the simulation results are already averaged across 1000 replications.
